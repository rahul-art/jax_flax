{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "multisteps-serialization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul-art/jax_flax/blob/main/multisteps_serialization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DWrjF7Kka8P"
      },
      "source": [
        "%%capture\n",
        "!pip install -U flax optax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtRiKPtWkoFY"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "\n",
        "from typing import Sequence\n",
        "from flax.training.train_state import TrainState\n",
        "from flax.training.checkpoints import save_checkpoint, restore_checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94qFnrfhk7Sz",
        "outputId": "427be9fe-bd99-4c13-8abc-c9cc6273964a"
      },
      "source": [
        "rng = jax.random.PRNGKey(842)\n",
        "rng, data_rng = jax.random.split(rng)\n",
        "x = jnp.array([[x, x] for x in range(64)], dtype=jnp.float32)\n",
        "y = jnp.sum(2*x + 1, axis=-1, keepdims=True)\n",
        "x = x + jax.random.normal(data_rng, x.shape)\n",
        "def data_gen():\n",
        "    yield x, y\n",
        "    \n",
        "class MLP(nn.Module):\n",
        "    features: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for feat in self.features[:-1]:\n",
        "            x = nn.relu(nn.Dense(feat)(x))\n",
        "        x = nn.Dense(self.features[-1])(x)\n",
        "        return x\n",
        "\n",
        "model = MLP([4, 1])\n",
        "params = model.init(jax.random.PRNGKey(0), x)\n",
        "\n",
        "optimizer = optax.adamw(0.01)\n",
        "optimizer = optax.MultiSteps(optimizer, 4)\n",
        "state = TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:3035: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qof2H076kyAu"
      },
      "source": [
        "def compute_loss(params, batch):\n",
        "    preds = state.apply_fn(params, batch[0])\n",
        "    targs = batch[1]\n",
        "    return jnp.mean((preds - targs)**2)\n",
        "\n",
        "grad_fn = jax.value_and_grad(compute_loss)\n",
        "\n",
        "def train_step(state, batch):\n",
        "    def compute_loss(params):\n",
        "        preds = state.apply_fn(params, batch[0])\n",
        "        targs = batch[1]\n",
        "        return jnp.mean((preds - targs)**2)\n",
        "    grad_fn = jax.value_and_grad(compute_loss)\n",
        "    \n",
        "    loss, grad = grad_fn(state.params)\n",
        "\n",
        "    new_state = state.apply_gradients(grads=grad)\n",
        "\n",
        "    metrics = {\"loss\": loss}\n",
        "\n",
        "    return new_state, metrics\n",
        "\n",
        "train_step = jax.jit(train_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgxhBxk2lVFU",
        "outputId": "29564535-32ac-41d4-df97-f55ae71a3e4f"
      },
      "source": [
        "for i in range(8):\n",
        "    batch = next(data_gen())\n",
        "    state, metrics = train_step(state, batch)\n",
        "    print(metrics[\"loss\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:3035: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "15254.637\n",
            "15254.637\n",
            "15254.637\n",
            "15254.637\n",
            "14935.168\n",
            "14935.168\n",
            "14935.168\n",
            "14935.168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mFKjfKkulf0M",
        "outputId": "26cb7379-4be8-4caa-ae2d-804ea1aad342"
      },
      "source": [
        "save_checkpoint('./_tmp/', state, 8, overwrite=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'_tmp/checkpoint_8'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUpY-Ri0lm5m",
        "outputId": "618d923c-2800-42cc-ec27-eae765335028"
      },
      "source": [
        "model = MLP([4, 1])\n",
        "params = model.init(jax.random.PRNGKey(0), x)\n",
        "\n",
        "optimizer = optax.adamw(0.01)\n",
        "optimizer = optax.MultiSteps(optimizer, 4)\n",
        "state = TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:3035: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eZWcsQlF4AL"
      },
      "source": [
        "from flax.serialization import from_state_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KSF-YDTlsmL"
      },
      "source": [
        "state_ckpt = restore_checkpoint('./_tmp/', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BpV_U-llkpf"
      },
      "source": [
        "state = from_state_dict(state, state_ckpt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "cVbX0zuWlxQi",
        "outputId": "da54a2e4-b404-4afc-fc30-3c4a7930b3a8"
      },
      "source": [
        "for i in range(8):\n",
        "    batch = next(data_gen())\n",
        "    state, metrics = train_step(state, batch)\n",
        "    print(metrics[\"loss\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:3035: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b026da3cedc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         donated_invars=donated_invars)\n\u001b[0m\u001b[1;32m    433\u001b[0m     \u001b[0mout_pytree_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1564\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1555\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_new_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, trace, fun, tracers, params)\u001b[0m\n\u001b[1;32m   1567\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m   \u001b[0mprocess_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_call_impl\u001b[0;34m(fun, device, backend, name, donated_invars, *args)\u001b[0m\n\u001b[1;32m    578\u001b[0m   compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,\n\u001b[0;32m--> 579\u001b[0;31m                                *unsafe_map(arg_spec, args))\n\u001b[0m\u001b[1;32m    580\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mmemoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m       \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_callable\u001b[0;34m(fun, device, backend, name, donated_invars, *arg_specs)\u001b[0m\n\u001b[1;32m    651\u001b[0m   \u001b[0mabstract_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munzip2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_specs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m   \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_to_jaxpr_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"jit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_final\u001b[0;34m(fun, in_avals, transform_name)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaxpr_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_subjaxpr_dynamic\u001b[0;34m(fun, main, in_avals)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \u001b[0min_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-32c441a9faa7>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(state, batch)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flax/training/train_state.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     updates, new_opt_state = self.tx.update(\n\u001b[0;32m---> 73\u001b[0;31m         grads, self.opt_state, self.params)\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mnew_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optax/_src/wrappers.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, grads, state, params)\u001b[0m\n\u001b[1;32m    261\u001b[0m     updates, new_state = jax.lax.cond(\n\u001b[0;32m--> 262\u001b[0;31m         state.mini_step < k_steps - 1, (), mid_step, (), final_step)\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/control_flow.py\u001b[0m in \u001b[0;36mcond\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_cond_with_per_branch_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/control_flow.py\u001b[0m in \u001b[0;36m_cond_with_per_branch_args\u001b[0;34m(pred, true_operand, true_fun, false_operand, false_fun)\u001b[0m\n\u001b[1;32m    738\u001b[0m                \u001b[0;32mlambda\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfalse_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                (true_operand, false_operand))\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/control_flow.py\u001b[0m in \u001b[0;36m_cond\u001b[0;34m(pred, true_fun, false_fun, operand)\u001b[0m\n\u001b[1;32m    697\u001b[0m                         \u001b[0mout_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_jaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_avals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m                         false_out_tree, false_jaxpr.out_avals)\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/control_flow.py\u001b[0m in \u001b[0;36m_check_tree_and_avals\u001b[0;34m(what, tree1, avals1, tree2, avals2)\u001b[0m\n\u001b[1;32m   1940\u001b[0m     raise TypeError(\n\u001b[0;32m-> 1941\u001b[0;31m         f\"{what} must have same type structure, got {tree1} and {tree2}.\")\n\u001b[0m\u001b[1;32m   1942\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypematch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavals1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavals2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: TypeError: true_fun and false_fun output must have same type structure, got PyTreeDef((CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(namedtuple[<class 'optax._src.wrappers.MultiStepsState'>], [*, *, [CustomNode(namedtuple[<class 'flax.serialization.ScaleByAdamState'>], [*, CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}])]), CustomNode(namedtuple[<class 'flax.serialization.EmptyState'>], []), CustomNode(namedtuple[<class 'flax.serialization.EmptyState'>], [])], CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}])]))) and PyTreeDef((CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(namedtuple[<class 'optax._src.wrappers.MultiStepsState'>], [*, *, [CustomNode(namedtuple[<class 'optax._src.transform.ScaleByAdamState'>], [*, CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(<...\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b026da3cedc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-32c441a9faa7>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(state, batch)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flax/training/train_state.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m     updates, new_opt_state = self.tx.update(\n\u001b[0;32m---> 73\u001b[0;31m         grads, self.opt_state, self.params)\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mnew_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     return self.replace(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optax/_src/wrappers.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, grads, state, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     updates, new_state = jax.lax.cond(\n\u001b[0;32m--> 262\u001b[0;31m         state.mini_step < k_steps - 1, (), mid_step, (), final_step)\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: true_fun and false_fun output must have same type structure, got PyTreeDef((CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(namedtuple[<class 'optax._src.wrappers.MultiStepsState'>], [*, *, [CustomNode(namedtuple[<class 'flax.serialization.ScaleByAdamState'>], [*, CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}])]), CustomNode(namedtuple[<class 'flax.serialization.EmptyState'>], []), CustomNode(namedtuple[<class 'flax.serialization.EmptyState'>], [])], CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}])]))) and PyTreeDef((CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(namedtuple[<class 'optax._src.wrappers.MultiStepsState'>], [*, *, [CustomNode(namedtuple[<class 'optax._src.transform.ScaleByAdamState'>], [*, CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(<class 'flax.core.froze..."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKp42qDQXldf"
      },
      "source": [
        "## A cripy solution :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_Oe0kV1YT0i"
      },
      "source": [
        "def _zeros_tree_like(inp_tree):\n",
        "    return jax.tree_map(jnp.zeros_like, inp_tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsx8ujnKX7YG"
      },
      "source": [
        "fake_updates = _zeros_tree_like(state.params)\n",
        "_, new_inner_opt_state = state.tx.inner_opt.update(fake_updates, state.opt_state.inner_opt_state, state.params)\n",
        "opt_state = state.opt_state\n",
        "new_opt_state = optax.MultiStepsState(mini_step=opt_state.mini_step, \n",
        "                                      gradient_step=opt_state.gradient_step, \n",
        "                                      inner_opt_state=new_inner_opt_state,\n",
        "                                      acc_grads=opt_state.acc_grads)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJaLm9GSaFdx"
      },
      "source": [
        "state = state.replace(opt_state=new_opt_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks6AyAv-Z_Xv",
        "outputId": "2c51ce6b-4cb9-453a-d612-0c25a00ba97b"
      },
      "source": [
        "for i in range(8):\n",
        "    batch = next(data_gen())\n",
        "    state, metrics = train_step(state, batch)\n",
        "    print(metrics[\"loss\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:3035: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14611.871\n",
            "14611.871\n",
            "14611.871\n",
            "14611.871\n",
            "14333.213\n",
            "14333.213\n",
            "14333.213\n",
            "14333.213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psGOTA9zT8jc"
      },
      "source": [
        "## Reinstantiating the components of inner_opt_state resolves the issue"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMdvvVQLObxX"
      },
      "source": [
        "inner_opt_state = state.opt_state.inner_opt_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4N3qrVAOk7z"
      },
      "source": [
        "def reinstantiate_states(opt_state):\n",
        "    new_state = []\n",
        "    for state in opt_state:\n",
        "        cls = getattr(optax, type(state).__name__)\n",
        "        new_state.append(cls(**{k:getattr(state, k) for k in state._fields}))\n",
        "    return new_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NXbXftuQbF_"
      },
      "source": [
        "new_inner_opt_state = reinstantiate_states(inner_opt_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Di2HDAXQRxWD"
      },
      "source": [
        "ms_state_dict = {k:getattr(state.opt_state, k) for k in state.opt_state._fields}\n",
        "ms_state_dict[\"inner_opt_state\"] = new_inner_opt_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFe8-n2ZRDjo"
      },
      "source": [
        "state = state.replace(opt_state=optax.MultiStepsState(**ms_state_dict))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsEGqXlAT2cm",
        "outputId": "2dcb0d01-ce02-4a0f-bfd1-6258c41ee6bf"
      },
      "source": [
        "for i in range(8):\n",
        "    batch = next(data_gen())\n",
        "    state, metrics = train_step(state, batch)\n",
        "    print(metrics[\"loss\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13954.496\n",
            "13954.496\n",
            "13954.496\n",
            "13954.496\n",
            "13620.805\n",
            "13620.805\n",
            "13620.805\n",
            "13620.805\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmxv8JRCIuh0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f7cd00c-3159-4bec-a1e2-4bc800265956"
      },
      "source": [
        "state.tx._opt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientTransformation(init=<function chain.<locals>.init_fn at 0x7ff2539e23b0>, update=<function chain.<locals>.update_fn at 0x7ff2539e2440>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvec67v0l3y2"
      },
      "source": [
        "## with HF model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkXESrtvmoro"
      },
      "source": [
        "%%capture\n",
        "!pip install -U transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it6jGWZtlz6b"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "\n",
        "from typing import Sequence\n",
        "from flax.training.train_state import TrainState\n",
        "from flax.training.common_utils import onehot\n",
        "from flax.training.checkpoints import save_checkpoint, restore_checkpoint\n",
        "from transformers import FlaxAutoModelForCausalLM, AutoTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzSUeWNu6qBY"
      },
      "source": [
        "from flax.serialization import to_bytes, from_bytes\n",
        "import os\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALPz-n308J2n"
      },
      "source": [
        "def mb_item(x):\n",
        "    return x.item() if hasattr(x, \"item\") else x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9altzq16nbi"
      },
      "source": [
        "#checkpoint functions\n",
        "def save_model_checkpoint(model, save_dir, state, with_opt:bool=True, push_to_hub:bool=False):\n",
        "    \"\"\"\n",
        "    If `push_to_hub` is True, will save to `save_dir`. Otherwise will save to `save_dir/ckpt-{step}`.\n",
        "    \"\"\"\n",
        "    # state = jax_utils.unreplicate(state)\n",
        "    # logger.info(f\"SAVING CHECKPOINT IN {save_dir}...\")\n",
        "    if not push_to_hub:\n",
        "        save_dir = f\"{save_dir}/ckpt-{mb_item(state.step)-1}\"\n",
        "    model.save_pretrained(\n",
        "        save_dir,\n",
        "        params=state.params,\n",
        "        push_to_hub=push_to_hub,\n",
        "        commit_message=f\"Saving weights and logs at step {mb_item(state.step)-1}\",\n",
        "    )\n",
        "    if with_opt:\n",
        "        with open(os.path.join(save_dir, \"opt_state.msgpack\"), \"wb\") as f:\n",
        "            f.write(to_bytes(state.opt_state))\n",
        "        with open(os.path.join(save_dir, \"training_state.json\"), \"w\") as f:\n",
        "            json.dump({\"step\": state.step.item()}, f)\n",
        "    # logger.info(\"checkpoint saved\")\n",
        "\n",
        "\n",
        "def reinstantiate_states(opt_state):\n",
        "    new_state = []\n",
        "    for state in opt_state:\n",
        "        cls = getattr(optax, type(state).__name__)\n",
        "        new_state.append(cls(**{k:getattr(state, k) for k in state._fields}))\n",
        "    return new_state\n",
        "\n",
        "def restore_model_checkpoint(save_dir, state):\n",
        "    # logger.info(f\"RESTORING CHECKPOINT FROM {save_dir}...\")\n",
        "    with open(os.path.join(save_dir, \"flax_model.msgpack\"), \"rb\") as f:\n",
        "        params = from_bytes(state.params, f.read())\n",
        "\n",
        "    with open(os.path.join(save_dir, \"opt_state.msgpack\"), \"rb\") as f:\n",
        "        opt_state = from_bytes(state.opt_state, f.read())\n",
        "\n",
        "    with open(os.path.join(save_dir, \"training_state.json\"), \"r\") as f:\n",
        "        training_state = json.load(f)\n",
        "    step = training_state[\"step\"]\n",
        "\n",
        "    # logger.info(\"checkpoint restored\")\n",
        "    # reinstantiate inner opt state to avoid type conflict\n",
        "    if hasattr(opt_state, \"inner_opt_state\"):\n",
        "        print(\"restoring multisteps optimizer\")\n",
        "        inner_opt_state = reinstantiate_states(opt_state.inner_opt_state)\n",
        "        ms_state_dict = {k:getattr(state.opt_state, k) for k in state.opt_state._fields}\n",
        "        ms_state_dict[\"inner_opt_state\"] = inner_opt_state\n",
        "        opt_state = optax.MultiStepsState(**ms_state_dict)\n",
        "\n",
        "    return state.replace(step=step, params=params, opt_state=opt_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrgXh-ilmSLK"
      },
      "source": [
        "rng = jax.random.PRNGKey(842)\n",
        "rng, data_rng = jax.random.split(rng)\n",
        "input_ids = jax.random.randint(data_rng, (8, 128), 0, 256)\n",
        "labels = input_ids.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tVmjSxjmYoD",
        "outputId": "5221c5e4-46ad-4d75-d6e0-1621f951c034"
      },
      "source": [
        "model = FlaxAutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "optimizer = optax.adamw(1e-3)\n",
        "optimizer = optax.MultiSteps(optimizer, 4)\n",
        "\n",
        "state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:3035: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38QIUxXfmYln"
      },
      "source": [
        "def loss_fn(logits, labels):\n",
        "    shift_logits = logits[..., :-1, :]\n",
        "    shift_labels = labels[..., 1:]\n",
        "    loss = optax.softmax_cross_entropy(shift_logits, onehot(shift_labels, shift_logits.shape[-1]))\n",
        "    return loss.mean()\n",
        "\n",
        "def train_step(state, batch, rng):\n",
        "    dropout_rng, rng = jax.random.split(rng)\n",
        "    print(\"compiling...\")\n",
        "    def compute_loss(params):\n",
        "        labels = batch.pop(\"labels\")\n",
        "        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n",
        "        loss = loss_fn(logits, labels)\n",
        "        return loss\n",
        "\n",
        "    grad_fn = jax.value_and_grad(compute_loss)\n",
        "    loss, grad = grad_fn(state.params)\n",
        "\n",
        "    new_state = state.apply_gradients(grads=grad)\n",
        "\n",
        "    return new_state, loss, rng\n",
        "\n",
        "train_step = jax.jit(train_step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2OYTRypmYjS"
      },
      "source": [
        "def data_gen():\n",
        "    yield {\"input_ids\":input_ids, \"labels\":labels}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6_peZpcmYer",
        "outputId": "da507b43-5bd0-42cd-c296-16cf62bc1752"
      },
      "source": [
        "for i in range(24):\n",
        "    batch = next(data_gen())\n",
        "    state, loss, rng = train_step(state, batch, rng)\n",
        "    print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.4307\n",
            "8.407024\n",
            "8.3607645\n",
            "8.444751\n",
            "28.090708\n",
            "28.100212\n",
            "28.028742\n",
            "28.039408\n",
            "18.009052\n",
            "18.227276\n",
            "18.357918\n",
            "18.488722\n",
            "9.798409\n",
            "9.931963\n",
            "9.774198\n",
            "9.863355\n",
            "8.712209\n",
            "8.647808\n",
            "8.6835375\n",
            "8.694314\n",
            "7.514835\n",
            "7.5347486\n",
            "7.5292873\n",
            "7.491831\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xT6tD4qmkQt"
      },
      "source": [
        "save_model_checkpoint(model, \"./_hf\", state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAHHsSh98mtC"
      },
      "source": [
        "old_state = state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YPDM_SKmykE",
        "outputId": "aa2db03c-d9b1-4ef1-9b50-1e7bb0d2eff5"
      },
      "source": [
        "model = FlaxAutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "optimizer = optax.adamw(1e-3)\n",
        "optimizer = optax.MultiSteps(optimizer, 4)\n",
        "\n",
        "state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=optimizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:3035: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-YOH5zym3Ur",
        "outputId": "cb47e30a-07d2-4291-d1dc-7aad6e7253c7"
      },
      "source": [
        "state = restore_model_checkpoint(\"./_hf/ckpt-31\", state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "restoring multisteps optimizer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KRgA6tW86z7"
      },
      "source": [
        "def verify(state1, state2):\n",
        "    return jax.tree_multimap(lambda a,b: (a == b).all(), state1, state2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HDF8jbm9Nwr",
        "outputId": "35a97b55-1656-4b43-d7e2-0e7e8c99cfdf"
      },
      "source": [
        "verify(old_state.opt_state.inner_opt_state, state.opt_state.inner_opt_state)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ScaleByAdamState(count=DeviceArray(True, dtype=bool), mu={'transformer': {'h': {'0': {'attn': {'c_attn': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}, 'ln_1': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'ln_2': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'mlp': {'c_fc': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}}, '1': {'attn': {'c_attn': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}, 'ln_1': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'ln_2': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'mlp': {'c_fc': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}}, '2': {'attn': {'c_attn': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}, 'ln_1': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'ln_2': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'mlp': {'c_fc': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}}, '3': {'attn': {'c_attn': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}, 'ln_1': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'ln_2': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'mlp': {'c_fc': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}}, '4': {'attn': {'c_attn': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}, 'ln_1': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'ln_2': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'mlp': {'c_fc': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}}, '5': {'attn': {'c_attn': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}, 'ln_1': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'ln_2': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'mlp': {'c_fc': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}}}, 'ln_f': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'wpe': {'embedding': DeviceArray(True, dtype=bool)}, 'wte': {'embedding': DeviceArray(True, dtype=bool)}}}, nu={'transformer': {'h': {'0': {'attn': {'c_attn': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}, 'ln_1': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'ln_2': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'mlp': {'c_fc': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}}, '1': {'attn': {'c_attn': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}, 'ln_1': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'ln_2': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'mlp': {'c_fc': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}}, '2': {'attn': {'c_attn': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}, 'ln_1': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'ln_2': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'mlp': {'c_fc': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}}, '3': {'attn': {'c_attn': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}, 'ln_1': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'ln_2': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'mlp': {'c_fc': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}}, '4': {'attn': {'c_attn': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}, 'ln_1': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'ln_2': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'mlp': {'c_fc': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}}, '5': {'attn': {'c_attn': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}, 'ln_1': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'ln_2': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'mlp': {'c_fc': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}, 'c_proj': {'bias': DeviceArray(True, dtype=bool), 'kernel': DeviceArray(True, dtype=bool)}}}}, 'ln_f': {'bias': DeviceArray(True, dtype=bool), 'scale': DeviceArray(True, dtype=bool)}, 'wpe': {'embedding': DeviceArray(True, dtype=bool)}, 'wte': {'embedding': DeviceArray(True, dtype=bool)}}}),\n",
              " EmptyState(),\n",
              " EmptyState()]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZVQBiiNm6S-",
        "outputId": "768531a5-85cc-40a8-9fad-24cc9aaaa3a4"
      },
      "source": [
        "for i in range(16):\n",
        "    batch = next(data_gen())\n",
        "    state, loss, rng = train_step(state, batch, rng)\n",
        "    print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "compiling...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:3035: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7.0651155\n",
            "7.087094\n",
            "7.08247\n",
            "7.0764003\n",
            "6.989064\n",
            "7.010929\n",
            "6.989328\n",
            "7.0078263\n",
            "6.875692\n",
            "6.870382\n",
            "6.8747725\n",
            "6.8677526\n",
            "6.7281094\n",
            "6.713859\n",
            "6.739676\n",
            "6.72533\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUWu59LNniBa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsuaSnJ0F3vB"
      },
      "source": [
        "## One-go"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WUmzg7CrFzc1",
        "outputId": "deb6c984-22d3-4c0d-948a-a3198dd6d323"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import optax\n",
        "\n",
        "from typing import Sequence\n",
        "from flax.training.train_state import TrainState\n",
        "from flax.training.checkpoints import save_checkpoint, restore_checkpoint\n",
        "\n",
        "\n",
        "rng = jax.random.PRNGKey(842)\n",
        "rng, data_rng = jax.random.split(rng)\n",
        "x = jnp.array([[x, x] for x in range(64)], dtype=jnp.float32)\n",
        "y = jnp.sum(2*x + 1, axis=-1, keepdims=True)\n",
        "x = x + jax.random.normal(data_rng, x.shape)\n",
        "def data_gen():\n",
        "    yield x, y\n",
        "    \n",
        "class MLP(nn.Module):\n",
        "    features: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        for feat in self.features[:-1]:\n",
        "            x = nn.relu(nn.Dense(feat)(x))\n",
        "        x = nn.Dense(self.features[-1])(x)\n",
        "        return x\n",
        "\n",
        "model = MLP([4, 1])\n",
        "params = model.init(jax.random.PRNGKey(0), x)\n",
        "\n",
        "optimizer = optax.adamw(0.01)\n",
        "optimizer = optax.MultiSteps(optimizer, 4)\n",
        "state = TrainState.create(apply_fn=model.apply, params=params, tx=optimizer)\n",
        "\n",
        "\n",
        "def compute_loss(params, batch):\n",
        "    preds = state.apply_fn(params, batch[0])\n",
        "    targs = batch[1]\n",
        "    return jnp.mean((preds - targs)**2)\n",
        "\n",
        "grad_fn = jax.value_and_grad(compute_loss)\n",
        "\n",
        "def train_step(state, batch):\n",
        "    def compute_loss(params):\n",
        "        preds = state.apply_fn(params, batch[0])\n",
        "        targs = batch[1]\n",
        "        return jnp.mean((preds - targs)**2)\n",
        "    grad_fn = jax.value_and_grad(compute_loss)\n",
        "    \n",
        "    loss, grad = grad_fn(state.params)\n",
        "\n",
        "    new_state = state.apply_gradients(grads=grad)\n",
        "\n",
        "    metrics = {\"loss\": loss}\n",
        "\n",
        "    return new_state, metrics\n",
        "train_step = jax.jit(train_step)\n",
        "\n",
        "# train model, save checkpoint\n",
        "for i in range(8):\n",
        "    batch = next(data_gen())\n",
        "    state, metrics = train_step(state, batch)\n",
        "    print(metrics[\"loss\"])\n",
        "save_checkpoint('./_tmp/', state, 8, overwrite=True)\n",
        "\n",
        "# restore checkopint, resume training - fails\n",
        "state = restore_checkpoint('./_tmp/', state)\n",
        "for i in range(8):\n",
        "    batch = next(data_gen())\n",
        "    state, metrics = train_step(state, batch)\n",
        "    print(metrics[\"loss\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
            "/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py:3035: UserWarning: Explicitly requested dtype <class 'jax._src.numpy.lax_numpy.int64'> requested in zeros is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  lax._check_user_dtype_supported(dtype, \"zeros\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "15254.637\n",
            "15254.637\n",
            "15254.637\n",
            "15254.637\n",
            "14935.168\n",
            "14935.168\n",
            "14935.168\n",
            "14935.168\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-39e67e7720dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mcache_miss\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_fun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         donated_invars=donated_invars)\n\u001b[0m\u001b[1;32m    433\u001b[0m     \u001b[0mout_pytree_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1564\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_bind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mcall_bind\u001b[0;34m(primitive, fun, *args, **params)\u001b[0m\n\u001b[1;32m   1555\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mmaybe_new_sublevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_todos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_trace_todo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, trace, fun, tracers, params)\u001b[0m\n\u001b[1;32m   1567\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1568\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_call\u001b[0;34m(self, primitive, f, tracers, params)\u001b[0m\n\u001b[1;32m    608\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m   \u001b[0mprocess_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_call_impl\u001b[0;34m(fun, device, backend, name, donated_invars, *args)\u001b[0m\n\u001b[1;32m    578\u001b[0m   compiled_fun = _xla_callable(fun, device, backend, name, donated_invars,\n\u001b[0;32m--> 579\u001b[0;31m                                *unsafe_map(arg_spec, args))\n\u001b[0m\u001b[1;32m    580\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mmemoized_fun\u001b[0;34m(fun, *args)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m       \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36m_xla_callable\u001b[0;34m(fun, device, backend, name, donated_invars, *arg_specs)\u001b[0m\n\u001b[1;32m    651\u001b[0m   \u001b[0mabstract_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munzip2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_specs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m   \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_to_jaxpr_final\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabstract_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"jit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr_final\u001b[0;34m(fun, in_avals, transform_name)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaxpr_stack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1209\u001b[0;31m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_avals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_subjaxpr_dynamic\u001b[0;34m(fun, main, in_avals)\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \u001b[0min_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_avals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0min_tracers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m     \u001b[0mout_tracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-39e67e7720dd>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(state, batch)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flax/training/train_state.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m     updates, new_opt_state = self.tx.update(\n\u001b[0;32m---> 73\u001b[0;31m         grads, self.opt_state, self.params)\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mnew_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optax/_src/wrappers.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, grads, state, params)\u001b[0m\n\u001b[1;32m    261\u001b[0m     updates, new_state = jax.lax.cond(\n\u001b[0;32m--> 262\u001b[0;31m         state.mini_step < k_steps - 1, (), mid_step, (), final_step)\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/control_flow.py\u001b[0m in \u001b[0;36mcond\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    716\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_cond_with_per_branch_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/control_flow.py\u001b[0m in \u001b[0;36m_cond_with_per_branch_args\u001b[0;34m(pred, true_operand, true_fun, false_operand, false_fun)\u001b[0m\n\u001b[1;32m    738\u001b[0m                \u001b[0;32mlambda\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfalse_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                (true_operand, false_operand))\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/control_flow.py\u001b[0m in \u001b[0;36m_cond\u001b[0;34m(pred, true_fun, false_fun, operand)\u001b[0m\n\u001b[1;32m    697\u001b[0m                         \u001b[0mout_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_jaxpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_avals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m                         false_out_tree, false_jaxpr.out_avals)\n\u001b[0m\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/control_flow.py\u001b[0m in \u001b[0;36m_check_tree_and_avals\u001b[0;34m(what, tree1, avals1, tree2, avals2)\u001b[0m\n\u001b[1;32m   1940\u001b[0m     raise TypeError(\n\u001b[0;32m-> 1941\u001b[0;31m         f\"{what} must have same type structure, got {tree1} and {tree2}.\")\n\u001b[0m\u001b[1;32m   1942\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypematch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavals1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavals2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: TypeError: true_fun and false_fun output must have same type structure, got PyTreeDef((CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(namedtuple[<class 'optax._src.wrappers.MultiStepsState'>], [*, *, [CustomNode(namedtuple[<class 'flax.serialization.ScaleByAdamState'>], [*, CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}])]), CustomNode(namedtuple[<class 'flax.serialization.EmptyState'>], []), CustomNode(namedtuple[<class 'flax.serialization.EmptyState'>], [])], CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}])]))) and PyTreeDef((CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(namedtuple[<class 'optax._src.wrappers.MultiStepsState'>], [*, *, [CustomNode(namedtuple[<class 'optax._src.transform.ScaleByAdamState'>], [*, CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(<...\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-39e67e7720dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-39e67e7720dd>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(state, batch)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flax/training/train_state.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[0;34m(self, grads, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m     updates, new_opt_state = self.tx.update(\n\u001b[0;32m---> 73\u001b[0;31m         grads, self.opt_state, self.params)\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mnew_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     return self.replace(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optax/_src/wrappers.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, grads, state, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     updates, new_state = jax.lax.cond(\n\u001b[0;32m--> 262\u001b[0;31m         state.mini_step < k_steps - 1, (), mid_step, (), final_step)\n\u001b[0m\u001b[1;32m    263\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: true_fun and false_fun output must have same type structure, got PyTreeDef((CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(namedtuple[<class 'optax._src.wrappers.MultiStepsState'>], [*, *, [CustomNode(namedtuple[<class 'flax.serialization.ScaleByAdamState'>], [*, CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}])]), CustomNode(namedtuple[<class 'flax.serialization.EmptyState'>], []), CustomNode(namedtuple[<class 'flax.serialization.EmptyState'>], [])], CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}])]))) and PyTreeDef((CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(namedtuple[<class 'optax._src.wrappers.MultiStepsState'>], [*, *, [CustomNode(namedtuple[<class 'optax._src.transform.ScaleByAdamState'>], [*, CustomNode(<class 'flax.core.frozen_dict.FrozenDict'>[()], [{'params': {'Dense_0': {'bias': *, 'kernel': *}, 'Dense_1': {'bias': *, 'kernel': *}}}]), CustomNode(<class 'flax.core.froze..."
          ]
        }
      ]
    }
  ]
}